<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.4" />
<title>tests.test_settings API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tests.test_settings</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import time
import pytest

from common import (  # NOQA
    get_longhorn_api_client, get_self_host_id,
    get_core_api_client, get_apps_api_client,
    create_and_check_volume, cleanup_volume,
    wait_for_volume_healthy, wait_for_volume_detached,
    write_volume_random_data, check_volume_data,
    get_default_engine_image,
    wait_for_engine_image_state,
    wait_for_instance_manager_desire_state,
    generate_volume_name,
    wait_for_volume_condition_scheduled,
    client, core_api, settings_reset,
    apps_api, scheduling_api, priority_class, volume_name,

    LONGHORN_NAMESPACE, SETTING_TAINT_TOLERATION,
    RETRY_COUNTS, RETRY_INTERVAL_LONG, SETTING_GUARANTEED_ENGINE_CPU,
    SETTING_PRIORITY_CLASS, SIZE, RETRY_INTERVAL
)

from test_infra import wait_for_node_up_longhorn

KUBERNETES_DEFAULT_TOLERATION = &#34;kubernetes.io&#34;


def check_workload_update(core_api, apps_api, count):  # NOQA
    da_list = apps_api.list_namespaced_daemon_set(LONGHORN_NAMESPACE).items
    for da in da_list:
        if da.status.updated_number_scheduled != count:
            return False

    dp_list = apps_api.list_namespaced_deployment(LONGHORN_NAMESPACE).items
    for dp in dp_list:
        if dp.status.updated_replicas != dp.spec.replicas:
            return False

    im_pod_list = core_api.list_namespaced_pod(
        LONGHORN_NAMESPACE,
        label_selector=&#34;longhorn.io/component=instance-manager&#34;).items
    if len(im_pod_list) != 2 * count:
        return False

    for p in im_pod_list:
        if p.status.phase != &#34;Running&#34;:
            return False

    client = get_longhorn_api_client()  # NOQA
    images = client.list_engine_image()
    assert len(images) == 1
    if images[0].state != &#34;ready&#34;:
        return False

    return True


def wait_for_longhorn_node_ready():
    client = get_longhorn_api_client()  # NOQA

    ei = get_default_engine_image(client)
    ei_name = ei[&#34;name&#34;]

    wait_for_engine_image_state(client, ei_name, &#34;ready&#34;)

    node = get_self_host_id()
    wait_for_node_up_longhorn(node, client)

    return client, node


def test_setting_toleration():
    &#34;&#34;&#34;
    Test toleration setting

    1. Verify that cannot use Kubernetes tolerations for Longhorn setting
    2. Use &#34;key1=value1:NoSchedule; key2:NoExecute&#34; as toleration.
    3. Create a volume and attach it.
    4. Verify that cannot update toleration setting when any volume is attached
    5. Generate and write `data1` into the volume
    6. Detach the volume.
    7. Update setting `toleration` to toleration.
    8. Wait for all the Longhorn components to restart with new toleration
    9. Attach the volume again and verify the volume `data1`.
    10. Generate and write `data2` to the volume.
    11. Detach the volume.
    12. Clean the `toleration` setting.
    13. Wait for all the Longhorn components to restart with no toleration
    14. Attach the volume and validate `data2`.
    15. Generate and write `data3` to the volume.
    &#34;&#34;&#34;
    client = get_longhorn_api_client()  # NOQA
    apps_api = get_apps_api_client()  # NOQA
    core_api = get_core_api_client()  # NOQA
    count = len(client.list_node())

    setting = client.by_id_setting(SETTING_TAINT_TOLERATION)

    with pytest.raises(Exception) as e:
        client.update(setting,
                      value=KUBERNETES_DEFAULT_TOLERATION + &#34;:NoSchedule&#34;)
    assert &#34;is considered as the key of Kubernetes default tolerations&#34; \
           in str(e.value)
    with pytest.raises(Exception) as e:
        client.update(setting,
                      value=&#34;key1=value1:NoSchedule; key2:InvalidEffect&#34;)
    assert &#39;invalid effect&#39; in str(e.value)

    setting_value_str = &#34;key1=value1:NoSchedule; key2:NoExecute&#34;
    setting_value_dict = \
        {&#34;key1&#34;: {&#34;key&#34;: &#34;key1&#34;, &#34;value&#34;: &#34;value1&#34;,
                  &#34;operator&#34;: &#34;Equal&#34;, &#34;effect&#34;: &#34;NoSchedule&#34;},
         &#34;key2&#34;: {&#34;key&#34;: &#34;key2&#34;, &#34;value&#34;: None,
                  &#34;operator&#34;: &#34;Exists&#34;, &#34;effect&#34;: &#34;NoExecute&#34;}, }

    volume_name = &#34;test-toleration-vol&#34;  # NOQA
    volume = create_and_check_volume(client, volume_name)
    volume.attach(hostId=get_self_host_id())
    volume = wait_for_volume_healthy(client, volume_name)
    with pytest.raises(Exception) as e:
        client.update(setting, value=setting_value_str)
    assert &#39;cannot modify toleration setting before all volumes are detached&#39; \
           in str(e.value)

    data1 = write_volume_random_data(volume)
    check_volume_data(volume, data1)

    volume.detach()
    wait_for_volume_detached(client, volume_name)

    setting = client.update(setting, value=setting_value_str)
    assert setting.value == setting_value_str
    wait_for_toleration_update(core_api, apps_api, count, setting_value_dict)

    client, node = wait_for_longhorn_node_ready()

    volume = client.by_id_volume(volume_name)
    volume.attach(hostId=node)
    volume = wait_for_volume_healthy(client, volume_name)
    check_volume_data(volume, data1)
    data2 = write_volume_random_data(volume)
    check_volume_data(volume, data2)
    volume.detach()
    wait_for_volume_detached(client, volume_name)

    # cleanup
    setting_value_str = &#34;&#34;
    setting_value_dict = {}
    setting = client.by_id_setting(SETTING_TAINT_TOLERATION)
    setting = client.update(setting, value=setting_value_str)
    assert setting.value == setting_value_str
    wait_for_toleration_update(core_api, apps_api, count, setting_value_dict)

    client, node = wait_for_longhorn_node_ready()

    volume = client.by_id_volume(volume_name)
    volume.attach(hostId=node)
    volume = wait_for_volume_healthy(client, volume_name)
    check_volume_data(volume, data2)
    data3 = write_volume_random_data(volume)
    check_volume_data(volume, data3)

    cleanup_volume(client, volume)


def wait_for_toleration_update(core_api, apps_api, count, set_tolerations):  # NOQA
    updated = False

    for i in range(RETRY_COUNTS):
        time.sleep(RETRY_INTERVAL_LONG)
        updated = True

        if not check_workload_update(core_api, apps_api, count):
            updated = False
            continue

        pod_list = core_api.list_namespaced_pod(LONGHORN_NAMESPACE).items
        for p in pod_list:
            if p.status.phase != &#34;Running&#34; or \
                    not check_tolerations_set(p.spec.tolerations,
                                              set_tolerations):
                updated = False
                break
        if not updated:
            continue

        if updated:
            break

    assert updated


def check_tolerations_set(current_toleration_list, set_tolerations):
    current_tolerations = dict()
    for t in current_toleration_list:
        if KUBERNETES_DEFAULT_TOLERATION not in t.key:
            current_tolerations[t.key] = \
                {&#34;key&#34;: t.key, &#34;value&#34;: t.value,
                 &#34;operator&#34;: t.operator, &#34;effect&#34;: t.effect}

    return current_tolerations == set_tolerations


def test_setting_guaranteed_engine_cpu(client, core_api):  # NOQA
    &#34;&#34;&#34;
    Test setting Guaranteed Engine CPU

    Notice any change of the setting will result in all the instance manager
    recreated, no matter if there is any volume attached or not.

    1. Change the setting to `xxx`. Update setting should fail.
    2. Change the setting to `0.1`
    3. Wait for all the IM pods to recreated and become running
    4. Verify every IM pod has guaranteed CPU set correctly to 100m
    5. Change the setting to `0`
    6. Wait for all the IM pods to recreated and become running
    7. Verify every IM pod no longer has guaranteed CPU set
    8. Change the setting to `200m`
    9. Wait for all the IM pods to recreated and become running
    10. Verify every IM pod has guaranteed CPU set to 200m
    11. Change the setting to `10`, means 10 vcpus
    12. Wait for all the IM pods to recreated but unable to run
    13. Verify no IM pod can become running, with guaranteed CPU set to 10
    14. Change the setting to `0.25`
    15. Wait for all the IM pods to recreated and become running
    16. Verify every IM pod has guaranteed CPU set to 250m
    17. Create a volume, verify everything works as normal

    Note: use fixture to restore the setting into the original state
    &#34;&#34;&#34;
    # Get current guaranteed engine cpu setting and save it to org_setting
    setting = client.by_id_setting(SETTING_GUARANTEED_ENGINE_CPU)

    # Get current running instance managers
    instance_managers = client.list_instance_manager()

    # Set an invalid value, and it should return error
    with pytest.raises(Exception) as e:
        client.update(setting, value=&#34;xxx&#34;)
    assert &#34;with invalid &#34; + SETTING_GUARANTEED_ENGINE_CPU in \
           str(e.value)

    # Update guaranteed engine cpu setting to 0.1
    guaranteed_engine_cpu_setting_check(client, core_api, setting, &#34;0.1&#34;,
                                        &#34;100m&#34;, instance_managers,
                                        &#34;Running&#34;, True)

    # Update guaranteed engine cpu setting to 0
    guaranteed_engine_cpu_setting_check(client, core_api, setting, &#34;0&#34;,
                                        None, instance_managers,
                                        &#34;Running&#34;, True)

    # Update guaranteed engine cpu setting to 200m
    guaranteed_engine_cpu_setting_check(client, core_api, setting, &#34;200m&#34;,
                                        &#34;200m&#34;, instance_managers,
                                        &#34;Running&#34;, True)

    # Update guaranteed engine cpu setting to 10
    guaranteed_engine_cpu_setting_check(client, core_api, setting, &#34;10&#34;,
                                        None, instance_managers,
                                        &#34;Running&#34;, False)

    # Update guaranteed engine cpu setting to 0.25
    guaranteed_engine_cpu_setting_check(client, core_api, setting, &#34;0.25&#34;,
                                        &#34;250m&#34;, instance_managers,
                                        &#34;Running&#34;, True)

    # Create a volume to test
    vol_name = generate_volume_name()
    volume = create_and_check_volume(client, vol_name)
    volume.attach(hostId=get_self_host_id())
    volume = wait_for_volume_healthy(client, vol_name)
    assert len(volume.replicas) == 3

    data = write_volume_random_data(volume)
    check_volume_data(volume, data)


def guaranteed_engine_cpu_setting_check(client, core_api, setting,  # NOQA
                                        val, cpu_val,  # NOQA
                                        instance_managers,  # NOQA
                                        state, desire):  # NOQA
    &#34;&#34;&#34;
    We check if instance managers are in the desired state with
    correct setting
    desire is for reflect the state we are looking for.
    If desire is True, meanning we need the state to be the same.
    Otherwise, we are looking for the state to be different.
    e.g. &#39;Pending&#39;, &#39;OutofCPU&#39;, &#39;Terminating&#39; they are all &#39;Not Running&#39;.
    &#34;&#34;&#34;
    # Update guaranteed engine cpu setting
    client.update(setting, value=val)

    # Give sometime to k8s to update the instance manager status
    time.sleep(6 * RETRY_INTERVAL)

    for im in instance_managers:
        wait_for_instance_manager_desire_state(client, core_api,
                                               im.name, state, desire)

    if desire:
        # Verify guaranteed CPU set correctly
        for im in instance_managers:
            pod = core_api.read_namespaced_pod(name=im.name,
                                               namespace=LONGHORN_NAMESPACE)
            if cpu_val:
                assert (pod.spec.containers[0].resources.requests[&#39;cpu&#39;] ==
                        cpu_val)
            else:
                assert (not pod.spec.containers[0].resources.requests)


def test_setting_priority_class(core_api, apps_api, scheduling_api, priority_class, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test that the Priority Class setting is validated and utilized correctly.

    1. Verify that the name of a non-existent Priority Class cannot be used
    for the Setting.
    2. Create a new Priority Class in Kubernetes.
    3. Create and attach a Volume.
    4. Verify that the Priority Class Setting cannot be updated with an
    attached Volume.
    5. Generate and write `data1`.
    6. Detach the Volume.
    7. Update the Priority Class Setting to the new Priority Class.
    8. Wait for all the Longhorn workloads to restart with the new Priority
    Class.
    9. Attach the Volume and verify `data1`.
    10. Generate and write `data2`.
    11. Unset the Priority Class Setting.
    12. Wait for all the Longhorn workloads to restart with the new Priority
    Class.
    13. Attach the Volume and verify `data2`.
    14. Generate and write `data3`.
    &#34;&#34;&#34;
    client = get_longhorn_api_client()  # NOQA
    count = len(client.list_node())
    name = priority_class[&#39;metadata&#39;][&#39;name&#39;]
    setting = client.by_id_setting(SETTING_PRIORITY_CLASS)

    with pytest.raises(Exception) as e:
        client.update(setting, value=name)
    assert &#39;failed to get priority class &#39; in str(e.value)

    scheduling_api.create_priority_class(priority_class)

    volume = create_and_check_volume(client, volume_name)
    volume.attach(hostId=get_self_host_id())
    volume = wait_for_volume_healthy(client, volume_name)

    with pytest.raises(Exception) as e:
        client.update(setting, value=name)
    assert &#39;cannot modify priority class setting before all volumes are &#39; \
           &#39;detached&#39; in str(e.value)

    data1 = write_volume_random_data(volume)
    check_volume_data(volume, data1)

    volume.detach()
    wait_for_volume_detached(client, volume_name)

    setting = client.update(setting, value=name)
    assert setting.value == name

    wait_for_priority_class_update(core_api, apps_api, count, priority_class)

    client, node = wait_for_longhorn_node_ready()

    volume = client.by_id_volume(volume_name)
    volume.attach(hostId=node)
    volume = wait_for_volume_healthy(client, volume_name)
    check_volume_data(volume, data1)
    data2 = write_volume_random_data(volume)
    check_volume_data(volume, data2)
    volume.detach()
    wait_for_volume_detached(client, volume_name)

    setting = client.by_id_setting(SETTING_PRIORITY_CLASS)
    setting = client.update(setting, value=&#39;&#39;)
    assert setting.value == &#39;&#39;
    wait_for_priority_class_update(core_api, apps_api, count)

    client, node = wait_for_longhorn_node_ready()

    volume = client.by_id_volume(volume_name)
    volume.attach(hostId=node)
    volume = wait_for_volume_healthy(client, volume_name)
    check_volume_data(volume, data2)
    data3 = write_volume_random_data(volume)
    check_volume_data(volume, data3)

    cleanup_volume(client, volume)


def check_priority_class(pod, priority_class=None):  # NOQA
    if priority_class:
        return pod.spec.priority == priority_class[&#39;value&#39;] and \
               pod.spec.priority_class_name == \
               priority_class[&#39;metadata&#39;][&#39;name&#39;]
    else:
        return pod.spec.priority == 0 and pod.spec.priority_class_name == &#39;&#39;


def wait_for_priority_class_update(core_api, apps_api, count, priority_class=None):  # NOQA
    updated = False

    for i in range(RETRY_COUNTS):
        time.sleep(RETRY_INTERVAL_LONG)
        updated = True

        if not check_workload_update(core_api, apps_api, count):
            updated = False
            continue

        pod_list = core_api.list_namespaced_pod(LONGHORN_NAMESPACE).items
        for p in pod_list:
            if p.status.phase != &#34;Running&#34; and \
                    not check_priority_class(p, priority_class):
                updated = False
                break
        if not updated:
            continue

        if updated:
            break

    assert updated</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="tests.test_settings.check_priority_class"><code class="name flex">
<span>def <span class="ident">check_priority_class</span></span>(<span>pod, priority_class=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_priority_class(pod, priority_class=None):  # NOQA
    if priority_class:
        return pod.spec.priority == priority_class[&#39;value&#39;] and \
               pod.spec.priority_class_name == \
               priority_class[&#39;metadata&#39;][&#39;name&#39;]
    else:
        return pod.spec.priority == 0 and pod.spec.priority_class_name == &#39;&#39;</code></pre>
</details>
</dd>
<dt id="tests.test_settings.check_tolerations_set"><code class="name flex">
<span>def <span class="ident">check_tolerations_set</span></span>(<span>current_toleration_list, set_tolerations)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_tolerations_set(current_toleration_list, set_tolerations):
    current_tolerations = dict()
    for t in current_toleration_list:
        if KUBERNETES_DEFAULT_TOLERATION not in t.key:
            current_tolerations[t.key] = \
                {&#34;key&#34;: t.key, &#34;value&#34;: t.value,
                 &#34;operator&#34;: t.operator, &#34;effect&#34;: t.effect}

    return current_tolerations == set_tolerations</code></pre>
</details>
</dd>
<dt id="tests.test_settings.check_workload_update"><code class="name flex">
<span>def <span class="ident">check_workload_update</span></span>(<span>core_api, apps_api, count)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_workload_update(core_api, apps_api, count):  # NOQA
    da_list = apps_api.list_namespaced_daemon_set(LONGHORN_NAMESPACE).items
    for da in da_list:
        if da.status.updated_number_scheduled != count:
            return False

    dp_list = apps_api.list_namespaced_deployment(LONGHORN_NAMESPACE).items
    for dp in dp_list:
        if dp.status.updated_replicas != dp.spec.replicas:
            return False

    im_pod_list = core_api.list_namespaced_pod(
        LONGHORN_NAMESPACE,
        label_selector=&#34;longhorn.io/component=instance-manager&#34;).items
    if len(im_pod_list) != 2 * count:
        return False

    for p in im_pod_list:
        if p.status.phase != &#34;Running&#34;:
            return False

    client = get_longhorn_api_client()  # NOQA
    images = client.list_engine_image()
    assert len(images) == 1
    if images[0].state != &#34;ready&#34;:
        return False

    return True</code></pre>
</details>
</dd>
<dt id="tests.test_settings.guaranteed_engine_cpu_setting_check"><code class="name flex">
<span>def <span class="ident">guaranteed_engine_cpu_setting_check</span></span>(<span>client, core_api, setting, val, cpu_val, instance_managers, state, desire)</span>
</code></dt>
<dd>
<div class="desc"><p>We check if instance managers are in the desired state with
correct setting
desire is for reflect the state we are looking for.
If desire is True, meanning we need the state to be the same.
Otherwise, we are looking for the state to be different.
e.g. 'Pending', 'OutofCPU', 'Terminating' they are all 'Not Running'.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def guaranteed_engine_cpu_setting_check(client, core_api, setting,  # NOQA
                                        val, cpu_val,  # NOQA
                                        instance_managers,  # NOQA
                                        state, desire):  # NOQA
    &#34;&#34;&#34;
    We check if instance managers are in the desired state with
    correct setting
    desire is for reflect the state we are looking for.
    If desire is True, meanning we need the state to be the same.
    Otherwise, we are looking for the state to be different.
    e.g. &#39;Pending&#39;, &#39;OutofCPU&#39;, &#39;Terminating&#39; they are all &#39;Not Running&#39;.
    &#34;&#34;&#34;
    # Update guaranteed engine cpu setting
    client.update(setting, value=val)

    # Give sometime to k8s to update the instance manager status
    time.sleep(6 * RETRY_INTERVAL)

    for im in instance_managers:
        wait_for_instance_manager_desire_state(client, core_api,
                                               im.name, state, desire)

    if desire:
        # Verify guaranteed CPU set correctly
        for im in instance_managers:
            pod = core_api.read_namespaced_pod(name=im.name,
                                               namespace=LONGHORN_NAMESPACE)
            if cpu_val:
                assert (pod.spec.containers[0].resources.requests[&#39;cpu&#39;] ==
                        cpu_val)
            else:
                assert (not pod.spec.containers[0].resources.requests)</code></pre>
</details>
</dd>
<dt id="tests.test_settings.test_setting_guaranteed_engine_cpu"><code class="name flex">
<span>def <span class="ident">test_setting_guaranteed_engine_cpu</span></span>(<span>client, core_api)</span>
</code></dt>
<dd>
<div class="desc"><p>Test setting Guaranteed Engine CPU</p>
<p>Notice any change of the setting will result in all the instance manager
recreated, no matter if there is any volume attached or not.</p>
<ol>
<li>Change the setting to <code>xxx</code>. Update setting should fail.</li>
<li>Change the setting to <code>0.1</code></li>
<li>Wait for all the IM pods to recreated and become running</li>
<li>Verify every IM pod has guaranteed CPU set correctly to 100m</li>
<li>Change the setting to <code>0</code></li>
<li>Wait for all the IM pods to recreated and become running</li>
<li>Verify every IM pod no longer has guaranteed CPU set</li>
<li>Change the setting to <code>200m</code></li>
<li>Wait for all the IM pods to recreated and become running</li>
<li>Verify every IM pod has guaranteed CPU set to 200m</li>
<li>Change the setting to <code>10</code>, means 10 vcpus</li>
<li>Wait for all the IM pods to recreated but unable to run</li>
<li>Verify no IM pod can become running, with guaranteed CPU set to 10</li>
<li>Change the setting to <code>0.25</code></li>
<li>Wait for all the IM pods to recreated and become running</li>
<li>Verify every IM pod has guaranteed CPU set to 250m</li>
<li>Create a volume, verify everything works as normal</li>
</ol>
<p>Note: use fixture to restore the setting into the original state</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_setting_guaranteed_engine_cpu(client, core_api):  # NOQA
    &#34;&#34;&#34;
    Test setting Guaranteed Engine CPU

    Notice any change of the setting will result in all the instance manager
    recreated, no matter if there is any volume attached or not.

    1. Change the setting to `xxx`. Update setting should fail.
    2. Change the setting to `0.1`
    3. Wait for all the IM pods to recreated and become running
    4. Verify every IM pod has guaranteed CPU set correctly to 100m
    5. Change the setting to `0`
    6. Wait for all the IM pods to recreated and become running
    7. Verify every IM pod no longer has guaranteed CPU set
    8. Change the setting to `200m`
    9. Wait for all the IM pods to recreated and become running
    10. Verify every IM pod has guaranteed CPU set to 200m
    11. Change the setting to `10`, means 10 vcpus
    12. Wait for all the IM pods to recreated but unable to run
    13. Verify no IM pod can become running, with guaranteed CPU set to 10
    14. Change the setting to `0.25`
    15. Wait for all the IM pods to recreated and become running
    16. Verify every IM pod has guaranteed CPU set to 250m
    17. Create a volume, verify everything works as normal

    Note: use fixture to restore the setting into the original state
    &#34;&#34;&#34;
    # Get current guaranteed engine cpu setting and save it to org_setting
    setting = client.by_id_setting(SETTING_GUARANTEED_ENGINE_CPU)

    # Get current running instance managers
    instance_managers = client.list_instance_manager()

    # Set an invalid value, and it should return error
    with pytest.raises(Exception) as e:
        client.update(setting, value=&#34;xxx&#34;)
    assert &#34;with invalid &#34; + SETTING_GUARANTEED_ENGINE_CPU in \
           str(e.value)

    # Update guaranteed engine cpu setting to 0.1
    guaranteed_engine_cpu_setting_check(client, core_api, setting, &#34;0.1&#34;,
                                        &#34;100m&#34;, instance_managers,
                                        &#34;Running&#34;, True)

    # Update guaranteed engine cpu setting to 0
    guaranteed_engine_cpu_setting_check(client, core_api, setting, &#34;0&#34;,
                                        None, instance_managers,
                                        &#34;Running&#34;, True)

    # Update guaranteed engine cpu setting to 200m
    guaranteed_engine_cpu_setting_check(client, core_api, setting, &#34;200m&#34;,
                                        &#34;200m&#34;, instance_managers,
                                        &#34;Running&#34;, True)

    # Update guaranteed engine cpu setting to 10
    guaranteed_engine_cpu_setting_check(client, core_api, setting, &#34;10&#34;,
                                        None, instance_managers,
                                        &#34;Running&#34;, False)

    # Update guaranteed engine cpu setting to 0.25
    guaranteed_engine_cpu_setting_check(client, core_api, setting, &#34;0.25&#34;,
                                        &#34;250m&#34;, instance_managers,
                                        &#34;Running&#34;, True)

    # Create a volume to test
    vol_name = generate_volume_name()
    volume = create_and_check_volume(client, vol_name)
    volume.attach(hostId=get_self_host_id())
    volume = wait_for_volume_healthy(client, vol_name)
    assert len(volume.replicas) == 3

    data = write_volume_random_data(volume)
    check_volume_data(volume, data)</code></pre>
</details>
</dd>
<dt id="tests.test_settings.test_setting_priority_class"><code class="name flex">
<span>def <span class="ident">test_setting_priority_class</span></span>(<span>core_api, apps_api, scheduling_api, priority_class, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test that the Priority Class setting is validated and utilized correctly.</p>
<ol>
<li>Verify that the name of a non-existent Priority Class cannot be used
for the Setting.</li>
<li>Create a new Priority Class in Kubernetes.</li>
<li>Create and attach a Volume.</li>
<li>Verify that the Priority Class Setting cannot be updated with an
attached Volume.</li>
<li>Generate and write <code>data1</code>.</li>
<li>Detach the Volume.</li>
<li>Update the Priority Class Setting to the new Priority Class.</li>
<li>Wait for all the Longhorn workloads to restart with the new Priority
Class.</li>
<li>Attach the Volume and verify <code>data1</code>.</li>
<li>Generate and write <code>data2</code>.</li>
<li>Unset the Priority Class Setting.</li>
<li>Wait for all the Longhorn workloads to restart with the new Priority
Class.</li>
<li>Attach the Volume and verify <code>data2</code>.</li>
<li>Generate and write <code>data3</code>.</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_setting_priority_class(core_api, apps_api, scheduling_api, priority_class, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test that the Priority Class setting is validated and utilized correctly.

    1. Verify that the name of a non-existent Priority Class cannot be used
    for the Setting.
    2. Create a new Priority Class in Kubernetes.
    3. Create and attach a Volume.
    4. Verify that the Priority Class Setting cannot be updated with an
    attached Volume.
    5. Generate and write `data1`.
    6. Detach the Volume.
    7. Update the Priority Class Setting to the new Priority Class.
    8. Wait for all the Longhorn workloads to restart with the new Priority
    Class.
    9. Attach the Volume and verify `data1`.
    10. Generate and write `data2`.
    11. Unset the Priority Class Setting.
    12. Wait for all the Longhorn workloads to restart with the new Priority
    Class.
    13. Attach the Volume and verify `data2`.
    14. Generate and write `data3`.
    &#34;&#34;&#34;
    client = get_longhorn_api_client()  # NOQA
    count = len(client.list_node())
    name = priority_class[&#39;metadata&#39;][&#39;name&#39;]
    setting = client.by_id_setting(SETTING_PRIORITY_CLASS)

    with pytest.raises(Exception) as e:
        client.update(setting, value=name)
    assert &#39;failed to get priority class &#39; in str(e.value)

    scheduling_api.create_priority_class(priority_class)

    volume = create_and_check_volume(client, volume_name)
    volume.attach(hostId=get_self_host_id())
    volume = wait_for_volume_healthy(client, volume_name)

    with pytest.raises(Exception) as e:
        client.update(setting, value=name)
    assert &#39;cannot modify priority class setting before all volumes are &#39; \
           &#39;detached&#39; in str(e.value)

    data1 = write_volume_random_data(volume)
    check_volume_data(volume, data1)

    volume.detach()
    wait_for_volume_detached(client, volume_name)

    setting = client.update(setting, value=name)
    assert setting.value == name

    wait_for_priority_class_update(core_api, apps_api, count, priority_class)

    client, node = wait_for_longhorn_node_ready()

    volume = client.by_id_volume(volume_name)
    volume.attach(hostId=node)
    volume = wait_for_volume_healthy(client, volume_name)
    check_volume_data(volume, data1)
    data2 = write_volume_random_data(volume)
    check_volume_data(volume, data2)
    volume.detach()
    wait_for_volume_detached(client, volume_name)

    setting = client.by_id_setting(SETTING_PRIORITY_CLASS)
    setting = client.update(setting, value=&#39;&#39;)
    assert setting.value == &#39;&#39;
    wait_for_priority_class_update(core_api, apps_api, count)

    client, node = wait_for_longhorn_node_ready()

    volume = client.by_id_volume(volume_name)
    volume.attach(hostId=node)
    volume = wait_for_volume_healthy(client, volume_name)
    check_volume_data(volume, data2)
    data3 = write_volume_random_data(volume)
    check_volume_data(volume, data3)

    cleanup_volume(client, volume)</code></pre>
</details>
</dd>
<dt id="tests.test_settings.test_setting_toleration"><code class="name flex">
<span>def <span class="ident">test_setting_toleration</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Test toleration setting</p>
<ol>
<li>Verify that cannot use Kubernetes tolerations for Longhorn setting</li>
<li>Use "key1=value1:NoSchedule; key2:NoExecute" as toleration.</li>
<li>Create a volume and attach it.</li>
<li>Verify that cannot update toleration setting when any volume is attached</li>
<li>Generate and write <code>data1</code> into the volume</li>
<li>Detach the volume.</li>
<li>Update setting <code>toleration</code> to toleration.</li>
<li>Wait for all the Longhorn components to restart with new toleration</li>
<li>Attach the volume again and verify the volume <code>data1</code>.</li>
<li>Generate and write <code>data2</code> to the volume.</li>
<li>Detach the volume.</li>
<li>Clean the <code>toleration</code> setting.</li>
<li>Wait for all the Longhorn components to restart with no toleration</li>
<li>Attach the volume and validate <code>data2</code>.</li>
<li>Generate and write <code>data3</code> to the volume.</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_setting_toleration():
    &#34;&#34;&#34;
    Test toleration setting

    1. Verify that cannot use Kubernetes tolerations for Longhorn setting
    2. Use &#34;key1=value1:NoSchedule; key2:NoExecute&#34; as toleration.
    3. Create a volume and attach it.
    4. Verify that cannot update toleration setting when any volume is attached
    5. Generate and write `data1` into the volume
    6. Detach the volume.
    7. Update setting `toleration` to toleration.
    8. Wait for all the Longhorn components to restart with new toleration
    9. Attach the volume again and verify the volume `data1`.
    10. Generate and write `data2` to the volume.
    11. Detach the volume.
    12. Clean the `toleration` setting.
    13. Wait for all the Longhorn components to restart with no toleration
    14. Attach the volume and validate `data2`.
    15. Generate and write `data3` to the volume.
    &#34;&#34;&#34;
    client = get_longhorn_api_client()  # NOQA
    apps_api = get_apps_api_client()  # NOQA
    core_api = get_core_api_client()  # NOQA
    count = len(client.list_node())

    setting = client.by_id_setting(SETTING_TAINT_TOLERATION)

    with pytest.raises(Exception) as e:
        client.update(setting,
                      value=KUBERNETES_DEFAULT_TOLERATION + &#34;:NoSchedule&#34;)
    assert &#34;is considered as the key of Kubernetes default tolerations&#34; \
           in str(e.value)
    with pytest.raises(Exception) as e:
        client.update(setting,
                      value=&#34;key1=value1:NoSchedule; key2:InvalidEffect&#34;)
    assert &#39;invalid effect&#39; in str(e.value)

    setting_value_str = &#34;key1=value1:NoSchedule; key2:NoExecute&#34;
    setting_value_dict = \
        {&#34;key1&#34;: {&#34;key&#34;: &#34;key1&#34;, &#34;value&#34;: &#34;value1&#34;,
                  &#34;operator&#34;: &#34;Equal&#34;, &#34;effect&#34;: &#34;NoSchedule&#34;},
         &#34;key2&#34;: {&#34;key&#34;: &#34;key2&#34;, &#34;value&#34;: None,
                  &#34;operator&#34;: &#34;Exists&#34;, &#34;effect&#34;: &#34;NoExecute&#34;}, }

    volume_name = &#34;test-toleration-vol&#34;  # NOQA
    volume = create_and_check_volume(client, volume_name)
    volume.attach(hostId=get_self_host_id())
    volume = wait_for_volume_healthy(client, volume_name)
    with pytest.raises(Exception) as e:
        client.update(setting, value=setting_value_str)
    assert &#39;cannot modify toleration setting before all volumes are detached&#39; \
           in str(e.value)

    data1 = write_volume_random_data(volume)
    check_volume_data(volume, data1)

    volume.detach()
    wait_for_volume_detached(client, volume_name)

    setting = client.update(setting, value=setting_value_str)
    assert setting.value == setting_value_str
    wait_for_toleration_update(core_api, apps_api, count, setting_value_dict)

    client, node = wait_for_longhorn_node_ready()

    volume = client.by_id_volume(volume_name)
    volume.attach(hostId=node)
    volume = wait_for_volume_healthy(client, volume_name)
    check_volume_data(volume, data1)
    data2 = write_volume_random_data(volume)
    check_volume_data(volume, data2)
    volume.detach()
    wait_for_volume_detached(client, volume_name)

    # cleanup
    setting_value_str = &#34;&#34;
    setting_value_dict = {}
    setting = client.by_id_setting(SETTING_TAINT_TOLERATION)
    setting = client.update(setting, value=setting_value_str)
    assert setting.value == setting_value_str
    wait_for_toleration_update(core_api, apps_api, count, setting_value_dict)

    client, node = wait_for_longhorn_node_ready()

    volume = client.by_id_volume(volume_name)
    volume.attach(hostId=node)
    volume = wait_for_volume_healthy(client, volume_name)
    check_volume_data(volume, data2)
    data3 = write_volume_random_data(volume)
    check_volume_data(volume, data3)

    cleanup_volume(client, volume)</code></pre>
</details>
</dd>
<dt id="tests.test_settings.wait_for_longhorn_node_ready"><code class="name flex">
<span>def <span class="ident">wait_for_longhorn_node_ready</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_longhorn_node_ready():
    client = get_longhorn_api_client()  # NOQA

    ei = get_default_engine_image(client)
    ei_name = ei[&#34;name&#34;]

    wait_for_engine_image_state(client, ei_name, &#34;ready&#34;)

    node = get_self_host_id()
    wait_for_node_up_longhorn(node, client)

    return client, node</code></pre>
</details>
</dd>
<dt id="tests.test_settings.wait_for_priority_class_update"><code class="name flex">
<span>def <span class="ident">wait_for_priority_class_update</span></span>(<span>core_api, apps_api, count, priority_class=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_priority_class_update(core_api, apps_api, count, priority_class=None):  # NOQA
    updated = False

    for i in range(RETRY_COUNTS):
        time.sleep(RETRY_INTERVAL_LONG)
        updated = True

        if not check_workload_update(core_api, apps_api, count):
            updated = False
            continue

        pod_list = core_api.list_namespaced_pod(LONGHORN_NAMESPACE).items
        for p in pod_list:
            if p.status.phase != &#34;Running&#34; and \
                    not check_priority_class(p, priority_class):
                updated = False
                break
        if not updated:
            continue

        if updated:
            break

    assert updated</code></pre>
</details>
</dd>
<dt id="tests.test_settings.wait_for_toleration_update"><code class="name flex">
<span>def <span class="ident">wait_for_toleration_update</span></span>(<span>core_api, apps_api, count, set_tolerations)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_toleration_update(core_api, apps_api, count, set_tolerations):  # NOQA
    updated = False

    for i in range(RETRY_COUNTS):
        time.sleep(RETRY_INTERVAL_LONG)
        updated = True

        if not check_workload_update(core_api, apps_api, count):
            updated = False
            continue

        pod_list = core_api.list_namespaced_pod(LONGHORN_NAMESPACE).items
        for p in pod_list:
            if p.status.phase != &#34;Running&#34; or \
                    not check_tolerations_set(p.spec.tolerations,
                                              set_tolerations):
                updated = False
                break
        if not updated:
            continue

        if updated:
            break

    assert updated</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tests" href="index.html">tests</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="tests.test_settings.check_priority_class" href="#tests.test_settings.check_priority_class">check_priority_class</a></code></li>
<li><code><a title="tests.test_settings.check_tolerations_set" href="#tests.test_settings.check_tolerations_set">check_tolerations_set</a></code></li>
<li><code><a title="tests.test_settings.check_workload_update" href="#tests.test_settings.check_workload_update">check_workload_update</a></code></li>
<li><code><a title="tests.test_settings.guaranteed_engine_cpu_setting_check" href="#tests.test_settings.guaranteed_engine_cpu_setting_check">guaranteed_engine_cpu_setting_check</a></code></li>
<li><code><a title="tests.test_settings.test_setting_guaranteed_engine_cpu" href="#tests.test_settings.test_setting_guaranteed_engine_cpu">test_setting_guaranteed_engine_cpu</a></code></li>
<li><code><a title="tests.test_settings.test_setting_priority_class" href="#tests.test_settings.test_setting_priority_class">test_setting_priority_class</a></code></li>
<li><code><a title="tests.test_settings.test_setting_toleration" href="#tests.test_settings.test_setting_toleration">test_setting_toleration</a></code></li>
<li><code><a title="tests.test_settings.wait_for_longhorn_node_ready" href="#tests.test_settings.wait_for_longhorn_node_ready">wait_for_longhorn_node_ready</a></code></li>
<li><code><a title="tests.test_settings.wait_for_priority_class_update" href="#tests.test_settings.wait_for_priority_class_update">wait_for_priority_class_update</a></code></li>
<li><code><a title="tests.test_settings.wait_for_toleration_update" href="#tests.test_settings.wait_for_toleration_update">wait_for_toleration_update</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.4</a>.</p>
</footer>
</body>
</html>